# 1. 读取文本文件
text="""abcdefghijklmnopqrstuvwxyz ABCDEFGHIJKLMNOPQRSTUVWXYZ"""

print(f"文本长度：{len(text)} 个字符")
print(f"前100个字符：{text[:100]}")

# 2. 创建词汇表（所有不重复的字符）

chars = sorted(list(set(text)))  # set()去重，sorted()排序
vocab_size = len(chars)  # 词汇表大小
print(f"词汇表大小：{vocab_size}")
print(f"词汇表前10个字符：{chars[:10]}")

# 3. 创建编码器（字符→整数）和解码器（整数→字符）
# 创建两个字典：字符→索引，索引→字符

stoi = {ch: i for i, ch in enumerate(chars)}  # 字符到整数
itos = {i: ch for i, ch in enumerate(chars)}  # 整数到字符

# 打印几个例子看看
print(f"'a'的编码：{stoi.get('a', '不在词汇表中')}")
print(f"编码65对应的字符：{itos.get(65, '超出范围')}")


def encode(s):
    """将字符串转换为整数列表"""

    return [stoi[c] for c in s]  # 行内注释：这是列表推导式，相当于循环


def decode(l):

    return ''.join([itos[i] for i in l])  # 行内注释：join连接列表元素


# 测试编码解码
test_str = "have a try"
encoded = encode(test_str)
decoded = decode(encoded)

print(f"原始字符串: {test_str}")
print(f"编码结果: {encoded}")
print(f"解码结果: {decoded}")
print(f"是否一致: {test_str == decoded}")

# 将数据转换为PyTorch张量
import torch

data = torch.tensor(encode(text), dtype=torch.long)
print(f"数据张量形状: {data.shape}")

# 划分训练集和验证集
n = int(0.9 * len(data))
train_data = data[:n]
val_data = data[n:]


def get_batch(split, batch_size=4, block_size=8):
    """获取小批量数据"""
    data = train_data if split == 'train' else val_data

    # 随机选择起始位置
    ix = torch.randint(len(data) - block_size, (batch_size,))

    # 获取输入和标签
    x = torch.stack([data[i:i + block_size] for i in ix])
    y = torch.stack([data[i + 1:i + block_size + 1] for i in ix])

    return x, y


# 测试获取批次
xb, yb = get_batch('train')
print(f"输入批次形状: {xb.shape}")
print(f"标签批次形状: {yb.shape}")
print(f"输入示例: {xb[0]}")
print(f"对应的标签: {yb[0]}")
print(f"解码输入: {decode(xb[0].tolist())}")
print(f"解码标签: {decode(yb[0].tolist())}")

import torch.nn as nn
import torch.nn.functional as F

print("\n===词嵌入层===")

class EmbeddingDemo:
    def __init__(self,vocab_size,embedding_dim):
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
    def forward(self,idx):
        return self.embedding(idx)
print("测试嵌入层")
embedding_dim=16
embedding_layer=EmbeddingDemo(vocab_size,embedding_dim)
test_indices=torch.tensor([1,2,3])
embedded=embedding_layer.forward(test_indices)

print(f"输入索引:{test_indices}")
print(f"输入形状: {test_indices.shape}")
print(f"嵌入输出形状: {embedded.shape}")
print(f"嵌入矩阵的形状: {embedding_layer.embedding.weight.shape}")

print("\n===自注意力机制===")

class SimpleAttention:
    def __init__(self,head_size):
        self.head_size=head_size
    def forward(self,x):
        B,T,C= x.shape
        Q=x
        K=x
        V=x
        print(f"\n计算Q，K，V")
        print(f" Q形状: {Q.shape}, K形状: {K.shape}, V形状: {V.shape}")

        attn_scores=torch.matmul(Q,K.transpose(-2,-1))#计算注意力分数
        print("\n计算注意力分数 Q × Kᵀ")
        print(f"  Q × Kᵀ 形状: {attn_scores.shape}")

        #  缩放注意力分数
        # 公式 attn_scores = attn_scores / √d
        d = K.shape[-1]  # 键的维度
        attn_scores = attn_scores / (d ** 0.5)
        print(f"\n步骤3: 缩放注意力分数，除以 √d")
        print(f"  d = {d}, √d ≈ {d ** 0.5:.2f}")

        attn_weights = F.softmax(attn_scores, dim=-1)
        print(f"\n步骤4: 应用softmax得到注意力权重")
        print(f"  softmax沿最后一个维度(dim=-1)应用")
        print(f"  结果: 每行和为1，形状: {attn_weights.shape}")

        output = torch.matmul(attn_weights, V)
        print(f"\n步骤5: 加权求和 (注意力权重 × V)")
        print(f"  [B, T, T] × [B, T, C] -> [B, T, C]")
        print(f"  输出形状: {output.shape}")

        if T <= 5:  # 只在小序列上可视化
            print(f"\n注意力权重矩阵示例 (第一个样本):")
            print(attn_weights[0].detach().numpy().round(3))
            print(f"验证: 每行和 ≈ {attn_weights[0].sum(dim=-1)}")

        return output


# ============================================
# 11. 多头注意力机制
# ============================================
print("\n=== 步骤11：多头注意力机制 ===")


class MultiHeadAttention(nn.Module):
    """
    多头自注意力

    数学概念提醒:
        多头注意力 = 多个注意力头并行处理
        每个头关注不同的特征模式
        最后将所有头的输出合并
    """

    def __init__(self, n_embd, n_head):
        """
        初始化多头注意力

        参数:
            n_embd: 嵌入维度
            n_head: 注意力头的数量
        """
        super().__init__()
        self.n_head = n_head
        self.head_size = n_embd // n_head

        print(f"初始化多头注意力:")
        print(f"  总嵌入维度: n_embd = {n_embd}")
        print(f"  头数量: n_head = {n_head}")
        print(f"  每个头的维度: head_size = n_embd // n_head = {self.head_size}")

        # 确保整除
        assert n_embd % n_head == 0, "n_embd必须能被n_head整除"

        # 创建查询、键、值的线性变换
        # 数学提醒: 这些都是可学习的权重矩阵
        self.query = nn.Linear(n_embd, n_embd)
        self.key = nn.Linear(n_embd, n_embd)
        self.value = nn.Linear(n_embd, n_embd)

        # 输出投影
        self.proj = nn.Linear(n_embd, n_embd)

    def forward(self, x):
        """
        前向传播

        参数:
            x: 输入张量，形状 [B, T, C]

        返回:
            多头注意力输出，形状 [B, T, C]
        """
        B, T, C = x.shape

        # 步骤1: 计算查询、键、值
        # 公式提醒: Q = x × W_q, K = x × W_k, V = x × W_v
        Q = self.query(x)  # [B, T, n_embd]
        K = self.key(x)  # [B, T, n_embd]
        V = self.value(x)  # [B, T, n_embd]

        print(f"\n步骤1: 计算Q, K, V")
        print(f"  Q形状: {Q.shape}, K形状: {K.shape}, V形状: {V.shape}")

        # 步骤2: 重塑为多头
        # 从 [B, T, n_embd] 重塑为 [B, T, n_head, head_size]
        # 然后转置为 [B, n_head, T, head_size]
        Q = Q.view(B, T, self.n_head, self.head_size).transpose(1, 2)
        K = K.view(B, T, self.n_head, self.head_size).transpose(1, 2)
        V = V.view(B, T, self.n_head, self.head_size).transpose(1, 2)

        print(f"\n步骤2: 重塑为多头")
        print(f"  Q重塑后形状: {Q.shape}")
        print(f"  解释: 将嵌入维度分割成{n_head}个头")
        print(f"        每个头有{self.head_size}个维度")

        # 步骤3: 计算注意力
        # 公式提醒: 注意力 = softmax(Q × Kᵀ / √d) × V
        att = (Q @ K.transpose(-2, -1)) * (1.0 / (self.head_size ** 0.5))
        att = F.softmax(att, dim=-1)
        out = att @ V  # [B, n_head, T, head_size]

        print(f"\n步骤3: 计算注意力")
        print(f"  注意力分数形状: {att.shape}")
        print(f"  输出形状: {out.shape}")

        # 步骤4: 合并多头
        out = out.transpose(1, 2).contiguous().view(B, T, C)

        print(f"\n步骤4: 合并多头")
        print(f"  转置后形状: {out.shape}")
        print(f"  重塑后形状: {out.shape}")

        # 步骤5: 输出投影
        out = self.proj(out)

        print(f"\n步骤5: 输出投影")
        print(f"  最终输出形状: {out.shape}")

        return out


# 测试多头注意力
print("测试多头注意力机制:")

n_embd = 12  # 嵌入维度
n_head = 3  # 注意力头数量

# 创建测试输入
test_input = torch.randn(1, 5, n_embd)  # [B=1, T=5, C=n_embd]
print(f"输入形状: {test_input.shape}")

# 创建多头注意力层
mha = MultiHeadAttention(n_embd, n_head)
output = mha(test_input)

# ============================================
# 12. 前馈神经网络（Feed Forward Network）
# ============================================
print("\n=== 步骤12：前馈神经网络 ===")


class FeedForward(nn.Module):
    """
    前馈神经网络（每个位置独立处理）

    数学公式提醒:
        FFN(x) = max(0, xW₁ + b₁)W₂ + b₂
        通常使用GELU激活函数而不是ReLU
    """

    def __init__(self, n_embd):
        """
        初始化前馈网络

        参数:
            n_embd: 嵌入维度
        """
        super().__init__()

        # 创建一个两层网络，中间维度通常是4倍
        self.net = nn.Sequential(
            nn.Linear(n_embd, 4 * n_embd),  # 扩大维度
            nn.GELU(),  # 激活函数
            nn.Linear(4 * n_embd, n_embd),  # 恢复维度
        )

        print(f"前馈网络结构:")
        print(f"  输入维度: {n_embd}")
        print(f"  隐藏层维度: {4 * n_embd} (4倍放大)")
        print(f"  输出维度: {n_embd}")

    def forward(self, x):
        """
        前向传播

        参数:
            x: 输入张量，形状 [B, T, C]

        返回:
            前馈网络输出，形状 [B, T, C]
        """
        return self.net(x)


# 测试前馈网络
print("测试前馈网络:")

ffn = FeedForward(n_embd)
test_input = torch.randn(1, 3, n_embd)  # [B=1, T=3, C=n_embd]

print(f"输入形状: {test_input.shape}")
output = ffn(test_input)
print(f"输出形状: {output.shape}")

# ============================================
# 13. Transformer块（完整版）
# ============================================
print("\n=== 步骤13：Transformer块 ===")


class TransformerBlock(nn.Module):
    """
    Transformer块

    数学概念提醒:
        Transformer块 = 多头注意力 + 前馈网络
        使用层归一化(LayerNorm)和残差连接
        公式: 输出 = 输入 + 子层(层归一化(输入))
    """

    def __init__(self, n_embd, n_head):
        """
        初始化Transformer块

        参数:
            n_embd: 嵌入维度
            n_head: 注意力头数量
        """
        super().__init__()

        print(f"初始化Transformer块:")
        print(f"  嵌入维度: {n_embd}")
        print(f"  注意力头数量: {n_head}")

        # 层归一化（注意力前）
        self.ln1 = nn.LayerNorm(n_embd)

        # 多头注意力
        self.attn = MultiHeadAttention(n_embd, n_head)

        # 层归一化（前馈网络前）
        self.ln2 = nn.LayerNorm(n_embd)

        # 前馈网络
        self.ffwd = FeedForward(n_embd)

    def forward(self, x):
        """
        前向传播

        参数:
            x: 输入张量，形状 [B, T, C]

        返回:
            Transformer块输出，形状 [B, T, C]
        """
        print(f"\n=== Transformer块前向传播开始 ===")
        print(f"输入形状: {x.shape}")

        # 第一个残差连接：注意力
        print(f"\n步骤1: 多头注意力 + 残差连接")
        print(f"  残差连接公式: x = x + 子层(层归一化(x))")

        # 数学提醒: 残差连接帮助梯度流动
        residual = x
        print(f"  保存残差: 形状 {residual.shape}")

        # 层归一化
        norm_x = self.ln1(x)
        print(f"  层归一化后: 形状 {norm_x.shape}")

        # 多头注意力
        attn_output = self.attn(norm_x)
        print(f"  注意力输出: 形状 {attn_output.shape}")

        # 残差连接
        x = x + attn_output
        print(f"  残差连接后: 形状 {x.shape}")
        print(f"  验证: {residual.shape} + {attn_output.shape} = {x.shape}")

        # 第二个残差连接：前馈网络
        print(f"\n步骤2: 前馈网络 + 残差连接")
        residual = x
        print(f"  保存残差: 形状 {residual.shape}")

        # 层归一化
        norm_x = self.ln2(x)
        print(f"  层归一化后: 形状 {norm_x.shape}")

        # 前馈网络
        ffwd_output = self.ffwd(norm_x)
        print(f"  前馈网络输出: 形状 {ffwd_output.shape}")

        # 残差连接
        x = x + ffwd_output
        print(f"  残差连接后: 形状 {x.shape}")
        print(f"  验证: {residual.shape} + {ffwd_output.shape} = {x.shape}")

        print(f"\n=== Transformer块前向传播结束 ===")
        return x


# 测试Transformer块
print("测试Transformer块:")

block = TransformerBlock(n_embd, n_head)
test_input = torch.randn(1, 5, n_embd)  # [B=1, T=5, C=n_embd]

print(f"输入形状: {test_input.shape}")
output = block(test_input)
print(f"最终输出形状: {output.shape}")

# ============================================
# 14. 完整GPT模型
# ============================================
print("\n=== 步骤14：完整GPT模型 ===")


class NanoGPT(nn.Module):
    """
    完整的NanoGPT模型

    数学概念回顾:
        1. 词嵌入: 将整数索引转换为向量
        2. 位置编码: 添加位置信息
        3. Transformer块: 多层处理
        4. 层归一化: 稳定训练
        5. 线性输出层: 预测下一个词
    """

    def __init__(self, vocab_size, n_embd=48, n_head=4, n_layer=3, block_size=128):
        """
        初始化GPT模型

        参数:
            vocab_size: 词汇表大小
            n_embd: 嵌入维度
            n_head: 注意力头数量
            n_layer: Transformer层数
            block_size: 最大序列长度
        """
        super().__init__()

        print(f"初始化NanoGPT:")
        print(f"  词汇表大小: {vocab_size}")
        print(f"  嵌入维度: {n_embd}")
        print(f"  注意力头数量: {n_head}")
        print(f"  Transformer层数: {n_layer}")
        print(f"  最大序列长度: {block_size}")

        # 词嵌入
        self.token_embedding = nn.Embedding(vocab_size, n_embd)
        print(f"\n1. 词嵌入层: {vocab_size}个词 -> {n_embd}维向量")

        # 位置嵌入
        self.position_embedding = nn.Embedding(block_size, n_embd)
        print(f"2. 位置嵌入层: 位置0-{block_size - 1} -> {n_embd}维向量")

        # Transformer块
        print(f"3. Transformer块: {n_layer}层")
        self.blocks = nn.Sequential(
            *[TransformerBlock(n_embd, n_head) for _ in range(n_layer)]
        )

        # 最终层归一化
        self.ln_f = nn.LayerNorm(n_embd)
        print(f"4. 最终层归一化: 形状 [*, {n_embd}]")

        # 语言模型头（线性层）
        self.lm_head = nn.Linear(n_embd, vocab_size)
        print(f"5. 语言模型头: {n_embd}维 -> {vocab_size}个词的概率")

        # 保存参数
        self.block_size = block_size
        self.n_embd = n_embd

    def forward(self, idx, targets=None):
        """
        前向传播

        参数:
            idx: 输入索引，形状 [B, T]
            targets: 目标索引，形状 [B, T]

        返回:
            logits: 预测分数，形状 [B, T, vocab_size]
            loss: 损失值（如果提供targets）
        """
        B, T = idx.shape

        print(f"\n=== 前向传播开始 ===")
        print(f"输入形状: {B}个批次 × {T}个词")

        # 检查序列长度
        if T > self.block_size:
            print(f"警告: 序列长度{T} > 最大长度{self.block_size}")
            idx = idx[:, :self.block_size]  # 截断
            T = self.block_size

        # 步骤1: 词嵌入
        print(f"\n步骤1: 词嵌入")
        tok_emb = self.token_embedding(idx)  # [B, T, n_embd]
        print(f"  词嵌入后形状: {tok_emb.shape}")

        # 步骤2: 位置嵌入
        print(f"\n步骤2: 位置嵌入")
        pos = torch.arange(T, device=idx.device)  # 位置0, 1, ..., T-1
        print(f"  位置序列: {pos.tolist()}")
        pos_emb = self.position_embedding(pos)  # [T, n_embd]
        print(f"  位置嵌入形状: {pos_emb.shape}")

        # 步骤3: 添加位置嵌入到词嵌入
        x = tok_emb + pos_emb  # [B, T, n_embd]
        print(f"\n步骤3: 词嵌入 + 位置嵌入")
        print(f"  {tok_emb.shape} + {pos_emb.shape} = {x.shape}")

        # 步骤4: 通过Transformer块
        print(f"\n步骤4: 通过Transformer块")
        x = self.blocks(x)
        print(f"  Transformer块后形状: {x.shape}")

        # 步骤5: 最终层归一化
        print(f"\n步骤5: 最终层归一化")
        x = self.ln_f(x)
        print(f"  层归一化后形状: {x.shape}")

        # 步骤6: 语言模型头（预测下一个词）
        print(f"\n步骤6: 语言模型头")
        logits = self.lm_head(x)  # [B, T, vocab_size]
        print(f"  预测分数(logits)形状: {logits.shape}")

        # 步骤7: 计算损失（如果提供targets）
        loss = None
        if targets is not None:
            print(f"\n步骤7: 计算损失")
            print(f"  目标形状: {targets.shape}")

            # 重塑logits和targets以计算交叉熵损失
            # 数学公式回顾: 交叉熵损失 = -Σ y_true × log(y_pred)
            B, T, V = logits.shape
            logits_flat = logits.view(B * T, V)
            targets_flat = targets.view(B * T)

            print(f"  重塑logits: {logits.shape} -> {logits_flat.shape}")
            print(f"  重塑targets: {targets.shape} -> {targets_flat.shape}")

            # 计算交叉熵损失
            loss = F.cross_entropy(logits_flat, targets_flat)
            print(f"  损失值: {loss.item():.4f}")

        print(f"\n=== 前向传播结束 ===")
        return logits, loss


# ============================================
# 15. 创建和测试模型
# ============================================
print("创建和测试NanoGPT模型:")

# 创建模型
model = NanoGPT(vocab_size=vocab_size, n_embd=24, n_head=2, n_layer=2, block_size=16)
print(f"\n模型总参数量: {sum(p.numel() for p in model.parameters())}")

# 生成测试数据
print(f"\n生成测试数据:")
xb, yb = get_batch('train', batch_size=2, block_size=10)
print(f"输入xb形状: {xb.shape}")
print(f"目标yb形状: {yb.shape}")

# 前向传播测试
print(f"\n进行前向传播测试:")
logits, loss = model(xb, yb)
print(f"最终损失: {loss.item():.4f}")

# ============================================
# 16. 文本生成函数
# ============================================
print("\n=== 步骤16：文本生成 ===")


def generate_text(model, context, max_new_tokens=10, temperature=1.0):
    """
    生成文本

    参数:
        model: 训练好的模型
        context: 初始上下文，形状 [1, T]
        max_new_tokens: 要生成的新词数量
        temperature: 控制随机性（>1更随机，<1更确定）

    数学概念:
        采样策略: 基于模型预测的概率分布选择下一个词
        温度参数: 调整概率分布的尖锐程度
    """
    print(f"开始文本生成:")
    print(f"  初始上下文: '{decode(context[0].tolist())}'")
    print(f"  长度: {context.shape[1]}")
    print(f"  最大新词数: {max_new_tokens}")
    print(f"  温度: {temperature}")

    model.eval()  # 切换到评估模式

    with torch.no_grad():  # 不计算梯度，节省内存
        for i in range(max_new_tokens):
            print(f"\n--- 第{i + 1}次生成 ---")

            # 如果上下文太长，截断到block_size
            if context.shape[1] > model.block_size:
                context = context[:, -model.block_size:]
                print(f"  截断上下文到 {model.block_size} 个词")

            # 获取预测
            print(f"  前向传播...")
            logits, _ = model(context)  # [B, T, vocab_size]

            # 只关注最后一个时间步的预测
            logits = logits[:, -1, :]  # [B, vocab_size]
            print(f"  最后一个时间步的logits形状: {logits.shape}")

            # 应用温度
            print(f"  应用温度 {temperature}")
            logits = logits / temperature

            # 转换为概率
            probs = F.softmax(logits, dim=-1)  # [B, vocab_size]
            print(f"  概率分布形状: {probs.shape}")

            # 采样下一个词
            idx_next = torch.multinomial(probs, num_samples=1)  # [B, 1]
            print(f"  采样结果: {idx_next.item()} -> '{decode([idx_next.item()])}'")

            # 添加到序列
            context = torch.cat([context, idx_next], dim=1)  # [B, T+1]
            print(f"  新序列长度: {context.shape[1]}")

            # 显示当前生成的文本
            current_text = decode(context[0].tolist())
            print(f"  当前文本: '{current_text}'")

    print(f"\n生成完成!")
    return decode(context[0].tolist())


# 测试文本生成
print("\n测试文本生成:")
context = torch.tensor([encode("Hello")], dtype=torch.long)
generated = generate_text(model, context, max_new_tokens=5)
print(f"\n最终生成的文本: '{generated}'")

# ============================================
# 17. 训练循环（简化版）
# ============================================
print("\n=== 步骤17：训练循环 ===")


def train_step(model, optimizer, xb, yb):
    """
    单个训练步骤

    数学回顾:
        1. 前向传播: 计算预测和损失
        2. 反向传播: 计算梯度
        3. 梯度下降: 更新参数
    """
    print(f"\n--- 训练步骤开始 ---")

    # 前向传播
    print(f"1. 前向传播")
    logits, loss = model(xb, yb)

    # 反向传播
    print(f"\n2. 反向传播")
    optimizer.zero_grad()  # 清除之前的梯度
    loss.backward()  # 计算梯度
    print(f"   损失值: {loss.item():.4f}")

    # 梯度下降
    print(f"\n3. 梯度下降")
    optimizer.step()  # 更新参数
    print(f"   参数已更新")

    print(f"\n--- 训练步骤结束 ---")
    return loss.item()


# 创建优化器
print("创建优化器:")
optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)
print(f"  优化器类型: AdamW")
print(f"  学习率: 3e-4")

# 执行几次训练步骤
print(f"\n执行训练步骤:")
for step in range(3):  # 只训练3步作为演示
    print(f"\n=== 第{step + 1}次训练 ===")
    xb, yb = get_batch('train', batch_size=2, block_size=8)
    loss = train_step(model, optimizer, xb, yb)
    print(f"损失: {loss:.4f}")

print("\n训练演示完成!")

# 将你之前的全部代码复制到这里（从文本读取到训练步骤）

# 确保模型已经训练（我们在这里加一个简单的训练循环）
print("\n=== 开始训练 ===")
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)
for step in range(50):  # 训练50步，可以根据需要增加
    xb, yb = get_batch('train', batch_size=4, block_size=8)
    logits, loss = model(xb, yb)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    if step % 10 == 0:
        print(f"Step {step}, loss: {loss.item():.4f}")
print("训练完成！")

# ========== 生成文本 ==========
print("\n=== 开始生成文本 ===")

# 定义初始上下文（字符串）
prompt = "yes"
print(f"初始提示: '{prompt}'")

# 将提示编码为张量
context = torch.tensor([encode(prompt)], dtype=torch.long)  # shape (1, len(prompt))

# 调用生成函数
generated = generate_text(
    model=model,
    context=context,
    max_new_tokens=50,      # 生成50个新字符
    temperature=0.8         # 控制随机性，值越低越保守
)

print(f"\n最终生成的文本:\n{generated}")
